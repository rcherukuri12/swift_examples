import TensorFlow

// defining a simple model
let hiddenSize: Int = 10

struct Model: Layer {
    var layer1 = Dense<Float>(inputSize: 4, outputSize: hiddenSize, activation: relu)
    var layer2 = Dense<Float>(inputSize: hiddenSize, outputSize: hiddenSize, activation: relu)
    var layer3 = Dense<Float>(inputSize: hiddenSize, outputSize: 3, activation: identity)
    
    @differentiable
    func callAsFunction(_ input: Tensor<Float>) -> Tensor<Float> {
        return input.sequenced(through: layer1, layer2, layer3)
    }
}
// intializing an optimizer
var classifier = Model()
let optimizer = SGD(for: classifier, learningRate: 0.02)
Context.local.learningPhase = .training
// Dummy data.
let x: Tensor<Float> = Tensor(randomNormal: [100, 4])
let y: Tensor<Int32> = Tensor(randomUniform: [100])

//Run a training loop
/* first approach 
for _ in 0..<1000 {
    let ùõÅmodel = gradient(at: classifier) { classifier -> Tensor<Float> in
        let ≈∑ = classifier(x)
        let loss = softmaxCrossEntropy(logits: ≈∑, labels: y)
        print("Loss: \(loss)")
        return loss
    }
    optimizer.update(&classifier, along: ùõÅmodel)
}
*/

// second approach : 
// 1. Make use of methods on Differentiable or Layer that produce a backpropagation function.
// 2. Compose your derivative computation.

for _ in 0..<1000 {
    let (≈∑, backprop) = classifier.appliedForBackpropagation(to: x)
    let (loss, ùõÅ≈∑) = valueWithGradient(at: ≈∑) { ≈∑ in softmaxCrossEntropy(logits: ≈∑, labels: y) }
    print("Method 2 : Model output: \(≈∑), Loss: \(loss)")
    let (ùõÅmodel, _) = backprop(ùõÅ≈∑)
    optimizer.update(&classifier, along: ùõÅmodel)
}